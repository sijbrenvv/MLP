{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import regex\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples \n",
    "np.random.seed(10)\n",
    "random.seed(10)\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9f116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data2021.csv')\n",
    "print(len(df))\n",
    "df = df.drop_duplicates(subset=['user_id']) # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d968c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['user_description'], inplace=True) # remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d786d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes emojis from given text (biographies)\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as string (emoji free biographies)\n",
    "    Example:\n",
    "        >>> give_emoji_free_text('Wiskunde lover, honden freak. üáÆüá±üáπüá≠üèÅüá©üá™üá≠üá∫üá≥üá±üá∑üá∏üá∏üáΩüá∏')\n",
    "        'Wiskunde lover, honden freak.'\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Description:\n",
    "        Removes URLs from given text (biographies)\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as string (URL free biographies)\n",
    "    Example:\n",
    "        >>> url_free_text('Columnist https://t.co/eX0BHLeWH2. Boosdrietig')\n",
    "        'Columnist Boosdrietig'\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f725468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Apply the give_emoji_free_text function and get biographies free of emojis\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def preprocessing(bio):\n",
    "    '''\n",
    "    Description:\n",
    "        Preprocesses input text by: removing: punctuation, emojis, URLs, Dutch and English stopwords, \\\n",
    "        tokenisation, lowercasing and lemmatisation.\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as list (preprocessed biographies)\n",
    "    Example:\n",
    "        >>> preprocessing('Wiskunde lover, honden freak. üáÆüá±üáπüá≠üèÅüá©üá™üá≠üá∫üá≥üá±üá∑üá∏üá∏üáΩüá∏')\n",
    "        [wiskunde, lover, honden, freak]\n",
    "    '''\n",
    "    \n",
    "    bio = \"\".join([i for i in bio if i not in (string.punctuation + '‚Ä¢')]) # Remove punctuation\n",
    "    bio = call_emoji_free(bio) # Remove emojis\n",
    "    bio = url_free_text(bio).rstrip() # Remove URLs\n",
    "    bio = word_tokenize(bio, language=\"dutch\") # Tokenise the biography\n",
    "    \n",
    "    bio = [item.lower() for item in bio] # lowercase\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "    bio = [i for i in bio if i not in stopwords] # Remove Dutch stopwords\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    bio = [i for i in bio if i not in stopwords] # Remove English stopwords\n",
    "    \n",
    "    bio = [wordnet_lemmatizer.lemmatize(word) for word in bio] # Lemmatise the biographies\n",
    "    \n",
    "    return bio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27553489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed'] = df['user_description'].apply(preprocessing)\n",
    "\n",
    "# Remove biographies which contain links only\n",
    "df = df[df.astype(str)['preprocessed'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer model and fit features\n",
    "sentences_list =  df['preprocessed'].values\n",
    "sentences = []\n",
    "for i in sentences_list:\n",
    "    detok = \" \".join(i)\n",
    "    sentences.append(detok)\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de77c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = MiniBatchKMeans(n_init = 3, random_state = seed, batch_size =3172)\n",
    "visualizer = KElbowVisualizer(model, k=(4,50))\n",
    "\n",
    "visualizer.fit(X.toarray())        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 51\n",
    "for n_clusters in range(5, k, 5):\n",
    "\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = MiniBatchKMeans(n_clusters = n_clusters, n_init = 3, random_state = seed, batch_size =3172)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer model and fit features\n",
    "sentences_list =  df['preprocessed'].values\n",
    "sentences = []\n",
    "for i in sentences_list:\n",
    "    detok = \" \".join(i)\n",
    "    sentences.append(detok)\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db89782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=seed)\n",
    "H = svd.fit(X)\n",
    "X = H.transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d86244",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = MiniBatchKMeans(n_init = 3, random_state = seed, batch_size =3172)\n",
    "visualizer = KElbowVisualizer(model, k=(4,50))\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834af835",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 51\n",
    "for n_clusters in range(5, k, 5):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = MiniBatchKMeans(n_clusters = n_clusters, n_init = 3, random_state = seed, batch_size =3172)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "km =KMeans(n_clusters = 5, n_init = 10, random_state=seed) # Initialise the k-means clusterer\n",
    "km = km.fit(X) # Compute k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f23f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = km.predict(X) # Compute cluster centers and predict cluster index for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = (km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de646bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate size of each cluster and group each by biography per cluster\n",
    "cluster_dict = {}\n",
    "freq_dict = {}\n",
    "for count, value in enumerate(sentences):\n",
    "    if label[count] in cluster_dict:\n",
    "        freq_dict[label[count]] += 1\n",
    "        cluster_dict[label[count]] += value\n",
    "    else:\n",
    "        freq_dict[label[count]] = 1\n",
    "        cluster_dict[label[count]] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 most frequent words per cluster and their size\n",
    "from collections import Counter\n",
    "cluster = {}\n",
    "name_list = []\n",
    "\n",
    "for i in cluster_dict:\n",
    "    wordsList = str(cluster_dict[i]).split()\n",
    "    wordsList = [ x for x in wordsList if x.isalpha() ]\n",
    "    counters = Counter(wordsList)\n",
    "    most_occur = counters.most_common(10)\n",
    "    n_words = []\n",
    "    for word in most_occur:\n",
    "        n_words.append(word[0])\n",
    "        \n",
    "    name_list.append(i)\n",
    "    print(n_words, \"Cluster:\", i, \"Size:\", freq_dict[i])\n",
    "    cluster[i] = n_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd82bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph with named labels\n",
    "cluster_size = list(freq_dict.values())\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(np.arange(len(name_list)), cluster_size, align='center')\n",
    "ax.set_yticks(np.arange(len(name_list)), labels=name_list)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('cluster size')\n",
    "ax.set_title('cluster size per cluster')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from: https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/\n",
    "def mbkmeans_clusters(\n",
    "    X, \n",
    "    k\n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=seed).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    \n",
    "    sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "    print(f\"Silhouette values:\")\n",
    "    silhouette_values = []\n",
    "    for i in range(k):\n",
    "        cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "        silhouette_values.append(\n",
    "            (\n",
    "                i,\n",
    "                cluster_silhouette_values.shape[0],\n",
    "                cluster_silhouette_values.mean(),\n",
    "                cluster_silhouette_values.min(),\n",
    "                cluster_silhouette_values.max(),\n",
    "            )\n",
    "        )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa24419",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbkmeans_clusters(X, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d0c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
