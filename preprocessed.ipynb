{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f007d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7bc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f49bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392446\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data2021.csv')\n",
    "print(len(df))\n",
    "df = df.drop_duplicates(subset=['user_id']) # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d3096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['user_description'], inplace=True) # remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb933677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         Wiskunde lover, honden freak. ğŸ‡®ğŸ‡±ğŸ‡¹ğŸ‡­ğŸğŸ‡©ğŸ‡ªğŸ‡­ğŸ‡ºğŸ‡³ğŸ‡±ğŸ‡·ğŸ‡¸ğŸ‡¸ğŸ‡½ğŸ‡¸...\n",
       "2         IT ICT IoT || â€¢AgriFoodTech â€¢Biobased â€¢Circula...\n",
       "6         Leesbaar, kroniek van gebeurtenissen in Amsterdam\n",
       "7         Mijn #TOTELTUIN ++ #Noseflutejob ++ Bach on th...\n",
       "8         Columnist https://t.co/eX0BHLeWH2. Boosdrietig...\n",
       "                                ...                        \n",
       "392401    Je suis un ğŸ¤– veillant sur la qualitÃ© de l'air ...\n",
       "392414    Voor het meest uitgebreide zakelijke nieuws be...\n",
       "392416    Uitgeverij Historische Verhalen publiceert kor...\n",
       "392422    Dutch,Jewish,single,had a great friend for lif...\n",
       "392442     Gitarist (klassiek) ğŸ¸ğŸ¼ğŸµğŸ¶ en ğŸ–¥ï¸ğŸ—ƒï¸ğŸ› ï¸ ğŸ˜ IT-Goeroe ğŸ˜\n",
       "Name: user_description, Length: 92363, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055fdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after : https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python #kingmakerking\n",
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes emojis from given text (biographies)\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as string (emoji free biographies)\n",
    "    Example:\n",
    "        >>> give_emoji_free_text('Wiskunde lover, honden freak. ğŸ‡®ğŸ‡±ğŸ‡¹ğŸ‡­ğŸğŸ‡©ğŸ‡ªğŸ‡­ğŸ‡ºğŸ‡³ğŸ‡±ğŸ‡·ğŸ‡¸ğŸ‡¸ğŸ‡½ğŸ‡¸')\n",
    "        'Wiskunde lover, honden freak.'\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Description:\n",
    "        Removes URLs from given text (biographies)\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as string (URL free biographies)\n",
    "    Example:\n",
    "        >>> url_free_text('Columnist https://t.co/eX0BHLeWH2. Boosdrietig')\n",
    "        'Columnist Boosdrietig'\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134aa89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Apply the give_emoji_free_text function and get biographies free of emojis\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def preprocessing(bio):\n",
    "    '''\n",
    "    Description:\n",
    "        Preprocesses input text by: removing: punctuation, emojis, URLs, Dutch and English stopwords, \\\n",
    "        tokenisation, lowercasing and lemmatisation.\n",
    "    Input:\n",
    "        Text as string (biographies)\n",
    "    Output:\n",
    "        Text as list (preprocessed biographies)\n",
    "    Example:\n",
    "        >>> preprocessing('Wiskunde lover, honden freak. ğŸ‡®ğŸ‡±ğŸ‡¹ğŸ‡­ğŸğŸ‡©ğŸ‡ªğŸ‡­ğŸ‡ºğŸ‡³ğŸ‡±ğŸ‡·ğŸ‡¸ğŸ‡¸ğŸ‡½ğŸ‡¸')\n",
    "        [wiskunde, lover, honden, freak]\n",
    "    '''\n",
    "    \n",
    "    bio = \"\".join([i for i in bio if i not in (string.punctuation + 'â€¢')]) # Remove punctuation\n",
    "    bio = call_emoji_free(bio) # Remove emojis\n",
    "    bio = url_free_text(bio).rstrip() # Remove URLs\n",
    "    bio = word_tokenize(bio, language=\"dutch\") # Tokenise the biography\n",
    "    \n",
    "    bio = [item.lower() for item in bio] # lowercase\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "    bio = [i for i in bio if i not in stopwords] # Remove Dutch stopwords\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    bio = [i for i in bio if i not in stopwords] # Remove English stopwords\n",
    "    \n",
    "    bio = [wordnet_lemmatizer.lemmatize(word) for word in bio] # Lemmatise the biographies\n",
    "    \n",
    "    return bio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ea5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed'] = df['user_description'].apply(preprocessing)\n",
    "\n",
    "# Remove biographies which contain links only\n",
    "df = df[df.astype(str)['preprocessed'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7907376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed'].to_csv('data2021_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64251d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
